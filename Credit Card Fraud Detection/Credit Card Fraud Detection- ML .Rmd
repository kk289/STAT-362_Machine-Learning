---
title: "Detect Credit Card Fraud with Machine Learning"
author: "Kevil Khadka"
date: "2/3/2020"
output:
  pdf_document: default
  html_document: default
---

## Library
```{r}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ranger)
library(caret)
library(data.table)
library(lattice)
setwd("/System/Volumes/Data/University of Evansville/SPRING 2020/STAT 493/Credit-card-dataset")
```

## Info
To detect credit card fraud, we use the card transcations dataset that contains a mix of fraud as well as non-fraudulent transcations. 

## Algorithms
- Decision Trees
- Logistic Regression
- Artificial Neural Networks
- Gradient Boosting Classifier

## Loading the dataset
```{r}
creditcard_data <- read.csv("creditcard.csv")
```

## Data Exploration

Display the dataset using the head() and tail() function. 

```{r}
dim(creditcard_data)
head(creditcard_data, 6)
tail(creditcard_data, 6)
```

```{r}
table(creditcard_data$Class)

summary(creditcard_data$Amount)

names(creditcard_data)

var(creditcard_data$Amount)
```

```{r}
sd(creditcard_data$Amount)
```

## Data Manipulation

- Scale the dataset using scale() function. Apply this to the AMOUNT variable of dataset. 
- Scaling is known as feature standardization. With this, the dataset is structured according to a specified range.

```{r}
head(creditcard_data)
```

```{r}
creditcard_data$Amount = scale(creditcard_data$Amount)

NewData = creditcard_data[,-c(1)]

head(NewData)
```

## Data Modeling

- Split the dataset into Training and Test set with split ratio of 0.80.
- Here, 80% of dataset will be attributed to the training data and 20% to test data.
- find the dimension using the dim() function

```{r}
library(caTools)
set.seed(123)

data_sample = sample.split(NewData$Class,SplitRatio = 0.80)

train_data = subset(NewData,data_sample == TRUE)

test_data = subset(NewData,data_sample == FALSE)

dim(train_data)
dim(test_data)
```

## Fitting Logistic Regression Model

- Logistic Regression model is used for modeling the outcome probability of a class i.e. fraud/not fraud.
- Implement the model on test data

```{r}
Logistic_Model = glm(Class~.,test_data,family=binomial())
summary(Logistic_Model)
```

```{r}
plot(Logistic_Model)
```

- To accesss performance of the model, we will delineate the ROC curve.
- ROC is Receiver Optimistic Characteristics

```{r}
library(pROC)

lr.predict <- predict(Logistic_Model, test_data, probability = TRUE)

myRoc <- roc(test_data$Class, lr.predict, plot = TRUE, col = "blue")
```

## Fitting a Decision Tree Model

- Implement a decision tree algorithm
- Decision tree to plot the outcomes of a decision. These outcomes are a consequences through which we can concludes as to what class the object belongs to.
- use the recursive parting to plot the decision tree.

```{r}
library(rpart)
library(rpart.plot)

decisionTree_model <- rpart(Class ~ . , creditcard_data, method = 'class')

predicted_val <- predict(decisionTree_model, creditcard_data, type = 'class')

probability <- predict(decisionTree_model, creditcard_data, type = 'prob')

rpart.plot(decisionTree_model)
```

## Artifical Neural Network (ANN)

- It is a type of machine learning algorithm that are modeled after the human nervous system.
- ANN model are able to learn the pattern using the historical data and are able to perform classification on the input data.
- Import the neuralnet package to implement ANNs.
- In case of ANN, there is a range of values that between 0 and 1.
- Set threshold as 0.5 i.e. values above 0.5 will correspond to 1, and rest will be 0.

```{r}
library(neuralnet)

ANN_model = neuralnet(Class~., train_data, linear.output=FALSE)

plot(ANN_model)

predANN = compute(ANN_model,test_data) 

resultANN = predANN$net.result
resultANN = ifelse(resultANN > 0.5,1,0)
```

## Gradient Boosting (GBM)

- It is a popular machine learning algorithm that is used to classification adn regression tasks.
- This model comprises of several underlying ensemble models like weak decision trees.
- This decision trees combine together to form a strong model of gradient boosting.

```{r}
library(gbm, quietly=TRUE)

# Get the time to train the GBM model
system.time(model_gbm <- gbm(Class ~ . , distribution = "bernoulli"
                             , data = rbind(train_data, test_data)
                             , n.trees = 500
                             , interaction.depth = 3
                             , n.minobsinnode = 100
                             , shrinkage = 0.01
                             , bag.fraction = 0.5
                             , train.fraction = nrow(train_data) / (nrow(train_data) + nrow(test_data))))

# Determine best iteration based on test data
gbm.iter = gbm.perf(model_gbm, method = "test")
```

```{r}
model.influence = relative.influence(model_gbm, n.trees = gbm.iter, sort. = TRUE)

# Plot the gbm model
plot(model_gbm)
```

```{r}
# Plot and calculate AUC on test data
gbm_test = predict(model_gbm, newdata = test_data, n.trees = gbm.iter)
gbm_auc = roc(test_data$Class, gbm_test, plot = TRUE, col = "red")
```

```{r}
print(gbm_auc)
```

## Summary

- Used a variety of Machine Learning alogrithms to implement this model
- Plotted the respective perfomance curves for the models
- Analyzed and visualized to discern fraudulent transcations from other types of data
