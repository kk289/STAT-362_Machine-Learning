---
title: "Titanic_dataset"
author: "Kevil Khadka"
date: "2/24/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(dplyr)
library(mice)
library(rpart)
library(rpart.plot)
```


## Decision Trees
Decision trees are versatile Machine Learning algorithm that can perform both classification and regression tasks. They are very powerful algorithms, capable of fitting complex datasets. Besides, decision trees are fundamental components of random forests, which are among the most potent Machine Learning algorithms available today.

## Purpose
The purpose of this dataset is to predict which people are more likely to survive after the collision with the iceberg.

## Import the dataset
```{r}
set.seed(678)
titanic <- read.csv("titanic_data.csv")
View(titanic)

titanic$age = as.double(titanic$age) 
titanic$fare = as.double(titanic$fare) 
```

```{r}
head(titanic)
```

```{r}
tail(titanic)
```

From the head and tail output, it seems the data is not shuffled. It is a big issue!. When we split data into train and test, we will select only the passenger from class 1 & 2 (No passenger from class 3 are in the top 80% of the observations), which means the algorithm will never see the features of passenger of class 3. This cloud lead to poor prediction.

To over the problem, we use the function sample()

```{r}
# Generate random list of index from 1 to 1309

shuffle_index <- sample(1:nrow(titanic))
head(shuffle_index)
```

```{r}
titanic <- titanic[shuffle_index, ]
head(titanic)
tail(titanic)
```

## Cleaning the dataset

- drop variables: x, name, ticket, cabin, and home.dest
- create factor variables for pclass and survived
- drop the NA

```{r}
titanic_2 <- titanic %>% 
  select(-c(x, name, ticket, cabin, home.dest)) %>% 
  mutate(pclass = factor(pclass, levels = c(1,2,3), labels = c('Upper', 'Middle', 'Lower')), 
         survived = factor(survived, levels = c(0,1), labels = c('Died', 'Saved'))) %>% 
  na.omit()

# View
glimpse(titanic_2)

# View(titanic_2)
is.na(titanic_2) 
md.pattern(titanic_2)
```

## Splitting data into train/test data
```{r}
create_train_test <- function(data, size = 0.8, train = TRUE){
  n_row = nrow(data)
  total_row = size * n_row
  train_sample <- (1:total_row)
  if (train == TRUE){
    return (data[train_sample, ])
    } else {
      return (data[-train_sample, ])
      }
}

# testing function
train_data <- create_train_test(titanic_2, 0.8, train = TRUE)
test_data <- create_train_test(titanic_2, 0.8, train = FALSE)

dim(train_data)
dim(test_data)
```

## To verify if the randomization process is correct.
Use the function prop.table() combined with table() to verify if the randomization process is correct.

```{r}
prop.table(table(train_data$survived))
prop.table(table(test_data$survived))

# In both dataset, the amount of survivors is the same, about 38 percent.
```

## Build the model

```{r}
fit <- rpart(survived ~ ., data = train_data, method = 'class')
rpart.plot(fit, type = 4, extra = "auto", nn = TRUE)
```


Note that, one of the many qualities of Decision Trees is that they require very little data preparation. In particular, they don't require feature scaling or centering. By default, rpart() function uses the Gini impurity measure to split the note. The higher the Gini coefficient, the more different instances within the node.

## Make a prediction

```{r}
predict_unseen <-predict(fit, test_data, type = 'class')
```

```{r}
# Testing the passenger who didn't make it and those who did.

table_mat <- table(test_data$survived, predict_unseen)
table_mat
```

## Measure Performance
```{r}
accuracy_test <- sum(diag(table_mat))/sum(table_mat)

print(paste('Accuracy for test', accuracy_test))

# It has score of 76.71% for the test dataset.
```


```{r}
# Make a prediction

predict_unseen_1 <-predict(fit, train_data, type = 'class')
```

```{r}
# Testing the passenger who didn't make it and those who did.

table_mat_1 <- table(train_data$survived, predict_unseen_1)
table_mat_1
```

```{r}
# Measure Performance
accuracy_train <- sum(diag(table_mat_1))/sum(table_mat_1)

print(paste('Accuracy for test', accuracy_train))

# It has score of 81.66% for the train dataset.
```


## Tune the hyper-parameters

Decision tree has various parameters that control aspects of the fit. In rpart library, we can control the parameters using the rpart.control() function. In the following code, we introduce the parameters we will tune. 

Proceed as follow:
- Construct function to return accuracy
- Tune the maximum depth
- Tune the minimum number of sample a node must have before it can split
- Tune the minimum number of sample a leaf node must have


```{r}
accuracy_tune <- function(fit) {
    predict_unseen <- predict(fit, test_data, type = 'class')
    table_mat <- table(test_data$survived, predict_unseen)
    accuracy_Test <- sum(diag(table_mat)) / sum(table_mat)
    accuracy_Test
}
```

We can try to tune the parameters and see if we can improve the model over the default value.
NOTE: We trying to get an accuracy higher than 0.7671 (TEST DATA).

```{r}
control <- rpart.control(minsplit = 4,
                         minbucket = round(5 / 3),
                         maxdepth = 3,
                         cp = 0)

tune_fit <- rpart(survived~., data = train_data, method = 'class', control = control)
accuracy_tune(tune_fit)
```

We get a little bit higher perfomance (0.7709) than previous model (0.7671). 